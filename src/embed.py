"""
embed.py - Generate embeddings for risk assessment descriptions using Azure OpenAI.

Reads risk_assessment_data.csv, calls the Azure OpenAI text-embedding-ada-002
deployment, and saves embeddings + metadata to output/embeddings.json.

Usage:
    python src/embed.py
"""

import json
import os
import sys
import time
from pathlib import Path

import pandas as pd
from dotenv import load_dotenv
from openai import AzureOpenAI

# ---------------------------------------------------------------------------
# Configuration
# ---------------------------------------------------------------------------

# Load .env from project root (one level up from src/)
PROJECT_ROOT = Path(__file__).resolve().parent.parent
load_dotenv(PROJECT_ROOT / ".env")

AZURE_ENDPOINT = os.getenv("AZURE_OPENAI_ENDPOINT")
AZURE_API_KEY = os.getenv("AZURE_OPENAI_API_KEY")
AZURE_DEPLOYMENT = os.getenv("AZURE_OPENAI_EMBEDDING_DEPLOYMENT")
AZURE_API_VERSION = os.getenv("AZURE_OPENAI_API_VERSION", "2024-02-01")

DATA_PATH = PROJECT_ROOT / "data" / "risk_assessment_data.csv"
OUTPUT_DIR = PROJECT_ROOT / "output"
OUTPUT_PATH = OUTPUT_DIR / "embeddings.json"

# ---------------------------------------------------------------------------
# Validation
# ---------------------------------------------------------------------------

def validate_config():
    """Check that all required environment variables are set."""
    missing = []
    if not AZURE_ENDPOINT:
        missing.append("AZURE_OPENAI_ENDPOINT")
    if not AZURE_API_KEY:
        missing.append("AZURE_OPENAI_API_KEY")
    if not AZURE_DEPLOYMENT:
        missing.append("AZURE_OPENAI_EMBEDDING_DEPLOYMENT")
    if missing:
        print(f"ERROR: Missing environment variables: {', '.join(missing)}")
        print("Make sure your .env file exists in the project root and was generated by Terraform.")
        sys.exit(1)

    if not DATA_PATH.exists():
        print(f"ERROR: Data file not found at {DATA_PATH}")
        print("Place risk_assessment_data.csv in the data/ directory.")
        sys.exit(1)

# ---------------------------------------------------------------------------
# Embedding logic
# ---------------------------------------------------------------------------

def get_embeddings(client: AzureOpenAI, texts: list[str], batch_size: int = 16) -> list[list[float]]:
    """
    Call Azure OpenAI embeddings API in batches.

    text-embedding-ada-002 supports batching up to 16 inputs per request.
    We add a small delay between batches to stay within rate limits
    (your Terraform deployed capacity = 1K TPM).
    """
    all_embeddings = []
    total_batches = (len(texts) + batch_size - 1) // batch_size

    for i in range(0, len(texts), batch_size):
        batch = texts[i : i + batch_size]
        batch_num = (i // batch_size) + 1
        print(f"  Batch {batch_num}/{total_batches} ({len(batch)} texts)...")

        response = client.embeddings.create(
            model=AZURE_DEPLOYMENT,
            input=batch,
        )

        batch_embeddings = [item.embedding for item in response.data]
        all_embeddings.extend(batch_embeddings)

        # Respect rate limits - your deployment is set to 1K TPM
        if batch_num < total_batches:
            time.sleep(2)

    return all_embeddings

# ---------------------------------------------------------------------------
# Main
# ---------------------------------------------------------------------------

def main():
    print("=" * 60)
    print("Azure OpenAI Embedding Generator")
    print("=" * 60)

    # Validate
    validate_config()
    print(f"\nEndpoint:   {AZURE_ENDPOINT}")
    print(f"Deployment: {AZURE_DEPLOYMENT}")
    print(f"API Ver:    {AZURE_API_VERSION}")

    # Load data
    print(f"\nLoading data from {DATA_PATH}...")
    df = pd.read_csv(DATA_PATH)
    print(f"Loaded {len(df)} risk records.")

    # Initialize Azure OpenAI client
    client = AzureOpenAI(
        azure_endpoint=AZURE_ENDPOINT,
        api_key=AZURE_API_KEY,
        api_version=AZURE_API_VERSION,
    )

    # Generate embeddings from the risk_description column
    print("\nGenerating embeddings...")
    descriptions = df["risk_description"].tolist()
    embeddings = get_embeddings(client, descriptions)
    print(f"Generated {len(embeddings)} embeddings (dimension: {len(embeddings[0])}).")

    # Build output structure
    output_data = {
        "model": AZURE_DEPLOYMENT,
        "dimension": len(embeddings[0]),
        "count": len(embeddings),
        "records": [],
    }

    for idx, row in df.iterrows():
        output_data["records"].append({
            "risk_id": row["risk_id"],
            "function": row["function"],
            "risk_owner": row["risk_owner"],
            "risk_description": row["risk_description"],
            "embedding": embeddings[idx],
        })

    # Save to output/
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    with open(OUTPUT_PATH, "w") as f:
        json.dump(output_data, f, indent=2)

    print(f"\nEmbeddings saved to {OUTPUT_PATH}")
    print(f"File size: {OUTPUT_PATH.stat().st_size / 1024:.1f} KB")
    print("\nDone! Run 'python src/cluster.py' next to cluster and visualize.")


if __name__ == "__main__":
    main()
